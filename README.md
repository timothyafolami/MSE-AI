# MSE-AI: Materials Science & Conversation Assistant

A dual-mode Streamlit-based chatbot application that combines general conversational AI with specialized materials science expertise.

## Features

### Dual-Mode Functionality
- **Conversational Mode**: Natural, friendly responses to everyday questions
- **Material Science Expert (MSE-AI) Mode**: In-depth technical support for materials science queries

### Intelligent Mode Detection
- Automatically determines whether a query is conversational or materials science-related
- Seamlessly switches between modes based on user input

### Material Science Expert Capabilities
- Step-by-step guided questioning process to gather key material requirements
- Initial and refined questions to deeply understand application needs
- Retrieval-Augmented Generation (RAG) using a vector database of materials science textbooks
- Generates detailed material recommendations with properties, tradeoffs, and reasoning
- Pre-indexed reference materials for quick, accurate retrieval

### Interactive User Experience
- Clean, intuitive Streamlit interface
- Sequential question flow to simplify the material selection process
- Ability to ask follow-up questions after receiving recommendations

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd MSE-AI
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up environment variables:
- Create a `.env` file in the project root
- Add your API key:
```
GROQ_API_KEY=your_groq_api_key_here
```

## Usage

1. First, index the reference documents:
```bash
python index_data.py
```

2. Run the application:
```bash
streamlit run main.py
```

3. Interact with the chatbot:
   - Ask any question - the system will determine whether it's conversational or materials-related
   - For material science queries:
     - Answer the 4 initial questions generated by the AI
     - Answer the 4 refined follow-up questions for more precision
     - Receive detailed material recommendations with justifications
   - For conversational queries, receive direct, friendly responses

## Project Structure

```
MSE-AI/
├── data/                   # Reference materials (PDFs)
├── logs/                   # Application logs
├── output/                 # Generated indices and outputs
│   └── doc_indexes/        # Document vector indices
├── src/                    # Source code
│   ├── ai_functions/       # AI prompt functions
│   │   ├── prompt_functions.py  # Core AI functionality
│   │   └── prompts.py           # Prompt templates
│   └── data_loader/        # Document processing modules
│       ├── doc_indexer.py       # Vector database indexing
│       ├── doc_loader.py        # Document loading utilities
│       ├── pdf_loader.py        # PDF processing
│       ├── settings.py          # Configuration settings
│       └── unstructured_loader.py  # Unstructured document handling
├── .env                    # Environment variables
├── index_data.py           # Script to index reference materials
├── main.py                 # Main application entry point
├── test_core_functions.py  # Test script for core functionality
└── requirements.txt        # Project dependencies
```

## Technical Implementation

- **LLM Integration**: Uses Llama and Qwen models via Groq API
- **Vector Database**: FAISS-based system for efficient similarity search
- **RAG Architecture**: Combines LLM capabilities with information retrieval
- **Embedding Model**: HuggingFace sentence transformers for document encoding
- **Streamlit Framework**: For interactive web application interface
- **Session State Management**: Maintains conversation flow and context

## Requirements

- Python 3.8+
- Streamlit
- LangChain
- GROQ API key
- FAISS for vector indexing
- HuggingFace Sentence Transformers

## Testing

Run the test script to verify functionality:
```bash
python test_core_functions.py
```

This will test both conversational and materials science modes.